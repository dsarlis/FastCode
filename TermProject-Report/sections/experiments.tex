\subsection{Graphs}

We used different graph files to measure the Sequential and MapRedude performace. The following table describe each one of them:

\begin{table}[h!]
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\bf Name} & {\bf Nodes}& {\bf Edges} & {\bf Size}\\
\hline
\hline
simple\_graph   & 10  & 14  & 55 bytes  \\
\hline
medium\_graph   & -  & -  & 96.2 Kb  \\
\hline
ego-Facebook   & 4039  & 88234  & 854.4 Kb  \\
\hline
ego-Gplus   & 107614  & 13673453  & 1.2GB  \\
\hline
big\_graph   & -  & -  & 2.4GB  \\
\hline
\end{tabular}
\caption{Graphs}
\label{tb:graphfiles}
\end{center}
\end{table}

All files consists of edges making simple to parse as showed in \ref{fig:graphfileformat}.

\begin{verbbox}
node_from_1 node_to_1
node_from_2 node_to_2
...
node_from_n node_to_n
\end{verbbox}
\begin{figure}[ht]
  \centering
  \theverbbox
  \caption{Graph file format}
  \label{fig:graphfileformat}
\end{figure}

\subsection{Sequential}
It was used an implementation of Tarjan\'s Algorithm that has $O(\mid V \mid + \mid E \mid)$ worst case performance. It uses a DFS keeping the time of the nodes beign first discovered and the order of oldest ancestor it can reach. It begins at an arbitrary node and  visits every node of the graph exactly once. As it is going through the graph it will be generating the different connected components.

We had to create our own file reader to load a graph into memory. One of the challenges we faced was the node mapping as nodes in the files had different ranges: file with $N$ nodes but they are not in the range $(0, N-1)$. Also, some nodes numbers could not be represented as long (bigger than $2^{63} -1$). In order to overcome this we had to use more memory to save the actual corresping value of each node.

\begin{table}[h!]
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
{\bf Graph} & {\bf Time (nanoseconds)}& {\bf \# of components} \\
\hline
\hline
simple\_graph   & 0  & 3  \\
\hline
medium\_graph   & 4  & 2   \\
\hline
ego-Facebook   & 11  & 1325  \\
\hline
ego-Gplus   & 812 & 37249 \\
\hline
big\_graph   & -  & -  \\
\hline
\end{tabular}
\caption{Sequential times}
\label{tb:sequentialtimes}
\end{center}
\end{table}

\subsection{MapReduce}

Two algorithm versions were implemented: Hast-to-All and Hash-to-Min. Both follow the same workflow:

\begin{algorithm}
\caption{Workflow}
\label{algo:workflow}
\begin{algorithmic}[1]
\State Initialize $C_{v}$ = ${v} \cup nbrs(v)$ for every $v \in V$
\Repeat
\State Emit $(u, C_u) \in h(C_ v)$
\State Merge $\bigcup C_v$ from $(v, C_v)$ 
\Until{$C_v does$ not change for all $v$}
\end{algorithmic}
\end{algorithm}

The initialization phase is done with one Map/Reduce job and each iteration of the cycle with another Map/Reduce job.

The Hash-to-All emits $(u, C_v)$ for all nodes $u \in C_v$.

The Hash-to-Min emits $(v_{min}, C_v)$ and $(u, v_{min})$ for all nodes $u \in C_v$.

The basic idea of the algorithm is to build up different components in a parallel fashion. It can be imagined as a different BFS being performed at the same time.

We represented the cluster as strings for an easy communication, but it adds overhead in the map and reduce functions as they will require to split the string and transform it. This were the results we obtained:

\begin{table}[h!]
\footnotesize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
{\bf Graph} & {\bf Time (nanoseconds)}& {\bf \# of components} \\
\hline
\hline
simple\_graph   & -  & -  \\
\hline
medium\_graph   & -  & -   \\
\hline
ego-Facebook   & -  & -  \\
\hline
ego-Gplus   & -  & -\\
\hline
big\_graph   & -  & -  \\
\hline
\end{tabular}
\caption{MapReduce times}
\label{tb:mapreducetimes}
\end{center}
\end{table}
