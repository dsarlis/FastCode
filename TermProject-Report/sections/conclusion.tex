In this work, we compared different kinds of algorithms for computing the connected components of a graph for different graph sizes varying from really small to quite big. The insights that we can summarize are that the running time of the algorithms depends on the structure of the graph as well as the mechanics of the algorithm itself and the way it transforms or not the original graph to compute the various components.

Opting over a distributed approach in finding connected components in a graph, is evidently faster for ever-increasing size of graphs. The algorithm fails to work sequentially on a large graph (with over 19 million nodes). The very same graph gets computed in only 33 minutes using \textit{Hadoop MapReduce} via \textit{AmazonWebServices}. With this we can conclude that for very large graphs, using map reduce paradigm gives a time-complexity of \textit{$O(logn)$} steps by parallel runs as compared to the traditional linear complexity of \textit{$O(|V| + |E|)$}, but if the input graph data-set is small then the sequential algorithm can save us from the overhead  (\eg Initialization of tasks/jobs, communication between mappers and reducers, generation of intermediate outputs, etc.) of map-reduce paradigm.

Further improvements on this work, include a number of various steps. First, a significant point is to try to reduce the memory footprint needed by Hash-to-Min algorithm, so it can be efficiently run on bigger graphs with hundreds of millions of nodes and edges. Monitoring tools such as Ganglia could be really helpful in this process, since they can report the amount of heap space used by the mapper and reducer tasks and allow for designing strategies to reduce the memory requirements of this algorithm. Furthermore, the Two-Phase algorithm needs to be refined to use a Distributed Hash Table to further reduce the number of rounds needed to converge. Another improvement in the Two-Phase algorithm would be the use of a distributed memory cache (such as Redis) to load the graph, \ie the edges, in memory. This would allow the reducers to directly check if the output they produce is different than the previous step and avoid relying on the checksums of files to find possible differences. Another research trajectory that we could follow is to apply the use of Graph databases on top of Map-Reduce and exploring how their particular features could help us improve the running time of the algorithms by utilizing their underlying infrastructure for improved graph-related operations.