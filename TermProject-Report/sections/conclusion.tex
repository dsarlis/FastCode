Opting over a distributed approach in finding connected components in a graph, is evidently faster for ever-increasing size of graphs. The algorithm fails to work sequentially on a large graph (estimated to have 200,000 nodes). The very same graph gets computed in only 33 minutes using \textit{Hadoop MapReduce} via \textit{AmazonWebServices}.

As discussed previously, the map reduce paradigm allows a time-complexity of \textit{$O(logn)$} steps which is faster than the traditional sequential complexity of \textit{$O(|V| + |E|)$}.

