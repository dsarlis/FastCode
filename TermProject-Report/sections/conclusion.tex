Opting over a distributed approach in finding connected components in a graph, is evidently faster for ever-increasing size of graphs. The algorithm fails to work sequentially on a large graph (with over 19 million nodes). The very same graph gets computed in only 33 minutes using \textit{Hadoop MapReduce} via \textit{AmazonWebServices}.

With this we can conclude that for very large graphs, using map reduce paradigm gives a time-complexity of \textit{$O(logn)$} steps by parallel runs as compared to the traditional linear complexity of \textit{$O(|V| + |E|)$}, but if the input graph data-set is small then the sequential algorithm can save us from the overhead  (\eg Initialization of tasks/jobs, communication between mappers and reducers, generation of intermediate outputs, etc.) of map-reduce paradigm.