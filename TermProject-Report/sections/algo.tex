The analysis that follows for both the sequential and MapReduce parts assumes a graph G that can be formalized as $G=(V,E)$ where $V$ is the set of vertices in the graph labeled from $0 \to N-1$ where $N=|V|$ and $E$ is the edges set, which is a set of of 2-element subsets in $V$, that is $E \subseteq \{\{u,v\}: \in V\}$.

\subsection{Sequential}
A sequential algorithm of calculating connected components is used as a proof of concept to show the differences in performance when using MapReduce and compare when to use one approach over the other.

We chose to implement a variation of Tarjan's algorithm to find strongly connected components. Although this algorithm calculates strongly connected components in directed graphs, it can be used to find connected components in undirected graphs as well. Algorithm \ref{algo:tarjan} provides the pseudocode of our implementation. It is based on \cite{tarjan}.

\begin{algorithm}
	\caption{Connected Components}
	\label{algo:tarjan}
	\begin{algorithmic}[1]
		\State Parse edges file
		\State Initialize graph as an adjacency list
		\State Initialize visited, stack, components
		\For {node $v \in nodes $}
			\If {$v \notin visited $}
				\State dfs($v$)
			\EndIf
		\EndFor
		\Function{dfs}{$v$}
			\State $visited[v] = true$
			\State $stack.push(v)$
			\State $isComponentRoot = true$
			\For {node $u \in neighbors(v)$}
				\If {$v \notin visited $}
					\State dfs($v$)
				\EndIf
				\If {$lowLink[u] > lowLink[v]$}
					\State $lowLink[u] = lowLink[v]$
					\State $isComponentRoot = false$
				\EndIf
			\EndFor
			\If {$isComponentRoot$}
				\State Construct component c from stack
				\State components.add(c)
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm's complexity is linear in the number of edges and nodes in the graph, \ie $O(|V| + |E|)$. This means that the algorithm will be very efficient for relatively small graphs, but it will start being uneffective while the graphs grow bigger and bigger up to hundreds of million vertices and edges. In particular, the parsing time of the input file would be the bottleneck since it has to be done sequentially as well as the available memory on the machine we are using, since the graph has to fit in its memory for Algorithm \ref{algo:tarjan} to work. MapReduce helps to eliminate exactly these restrictions, since the parsing can be performed in parallel in addition to running the actual connected components algorithm in a parallel fashion.

\subsection{Map Reduce}

Two algorithm versions were implemented: Hast-to-All and Hash-to-Min. Both follow the same workflow:

\begin{algorithm}
\caption{Workflow}
\label{algo:workflow}
\begin{algorithmic}[1]
\State Initialize $C_{v}$ = ${v} \cup nbrs(v)$ for every $v \in V$
\Repeat
\State Emit $(u, C_u) \in h(C_ v)$
\State Merge $\bigcup C_v$ from $(v, C_v)$
\Until{$C_v does$ not change for all $v$}
\end{algorithmic}
\end{algorithm}

The initialization phase is done with one Map/Reduce job and each iteration of the cycle with another Map/Reduce job.

The Hash-to-All emits $(u, C_v)$ for all nodes $u \in C_v$.

The Hash-to-Min emits $(v_{min}, C_v)$ and $(u, v_{min})$ for all nodes $u \in C_v$.

The basic idea of the algorithm is to build up different components in a parallel fashion. It can be imagined as a different BFS being performed at the same time.

