The analysis that follows for both the sequential and Map-Reduce parts assumes a graph G that can be formalized as $G=(V,E)$ where $V$ is the set of vertices in the graph labeled from $0 \to N-1$ where $N=|V|$ and $E$ is the edges set, which is a set of of 2-element subsets in $V$, that is $E \subseteq \{\{u,v\}: \in V\}$.

\subsection{Sequential}
A sequential algorithm of calculating connected components is used as a proof of concept to show the differences in performance when using Map-Reduce and compare when to use one approach over the other.

We chose to implement a variation of Tarjan's algorithm to find strongly connected components. Although this algorithm calculates strongly connected components in directed graphs, it can be used to find connected components in undirected graphs as well. Algorithm \ref{algo:tarjan} provides the pseudocode of our implementation. It is based on \cite{tarjan}.

\begin{algorithm}
	\caption{Connected Components}
	\label{algo:tarjan}
	\begin{algorithmic}[1]
		\State Parse edges file
		\State Initialize graph as an adjacency list
		\State Initialize visited, stack, components
		\For {node $v \in nodes $}
			\If {$v \notin visited $}
				\State dfs($v$)
			\EndIf
		\EndFor
		\Function{dfs}{$v$}
			\State $visited[v] = true$
			\State $stack.push(v)$
			\State $isComponentRoot = true$
			\For {node $u \in neighbors(v)$}
				\If {$v \notin visited $}
					\State dfs($v$)
				\EndIf
				\If {$lowLink[u] > lowLink[v]$}
					\State $lowLink[u] = lowLink[v]$
					\State $isComponentRoot = false$
				\EndIf
			\EndFor
			\If {$isComponentRoot$}
				\State Construct component c from stack
				\State components.add(c)
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The algorithm's complexity is linear in the number of edges and nodes in the graph, \ie $O(|V| + |E|)$. This means that the algorithm will be very efficient for relatively small graphs, but it will start being uneffective while the graphs grow bigger and bigger up to hundreds of million vertices and edges. In particular, the parsing time of the input file would be the bottleneck since it has to be done sequentially as well as the available memory on the machine we are using, since the graph has to fit in its memory for Algorithm \ref{algo:tarjan} to work. Map-Reduce helps to eliminate exactly these restrictions, since the parsing can be performed in parallel in addition to running the actual connected components algorithm in a parallel fashion.

\subsection{Map-Reduce}

The algorithm that we chose for the Map-Reduce part is based on the Hash-to-Min algorithm introduced in \cite{rastogi}. This algorithm works in multiple logarithmic Map-Reduce rounds  to calculate the connected components of a graph at the end. In each round the algorithm merges overlapping clusters to compute connected components. Essentially, the algorithm can be imagined as a different Breadth-First-Search (\eg BFS) being performed in parallel. Each BFS builds up a connected component by combining smaller ones until there is convergence and the whole connected component has been discovered.

\begin{algorithm}
	\caption{Initialization Step}
	\label{algo:first_step}
	\begin{algorithmic}[1]
		\Function{Map}{$key, value$}
			\State
		\EndFunction
		\Function{Reduce}{$key, List<value>$}
			\State
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}
	\caption{Iterative Map-Reduce step}
	\label{algo:second_step}
	\begin{algorithmic}[1]
		\Function{Map}{$key, value$}
			\State
		\EndFunction
		\Function{Reduce}{$key, List<value>$}
			\State
		\EndFunction
	\end{algorithmic}
\end{algorithm}

The initialization phase which parses the input file and creates the graph as an adjacency list is performed in one Map-Reduce job and each iteration of merging connected components is done using another Map-Reduce job.

The Hash-to-Min emits $(v_{min}, C_v)$ and $(u, v_{min})$ for all nodes $u \in C_v$.


